{"./":{"url":"./","title":"About SDFA","keywords":"","body":"What is SDFA SDFA is an efficient analysis tool designed for large-scale structural variation (SV) analysis. It is based on a new SV storage format and constructs a supporting toolset. Specifically, it first designs a standardized decomposition format (SDF) for SV, which efficiently represents, stores, and retrieves any type of SV data by decomposing SV. Based on the SDF file, SDFA designs or optimizes existing SV analysis algorithms considering the performance in large-scale samples. What is SDF The full name of SDF is Standardized Decomposition Format (SDF). It is a file format for splitting, storing, and compressing SV data: We provide a detailed explanation of the attributes in the above figure as follows: Group Field Value Type Description LOCATION coordinate int[3] The start and end positions of the chromosome where the current SV is located LOCATION length int The length of the current SV (for example, for an insertion variation, it is impossible to determine its length only relying on the coordinate field value) LOCATION type int Type of the current SV GENOTYPE genotypes bytecode The genotype of the current sample under this SV GENOTYPE metrics bytecodeList Quality metrics information of the current genotype VCF Field id bytecode The ID information of the current SV in the original VCF file VCF Field ref bytecode The REF information of the current SV in the original VCF file VCF Field alt bytecode The ALT information of the current SV in the original VCF file VCF Field qual bytecode The QUAL information of the current SV in the original VCF file VCF Field filter bytecode The FILTER information of the current SV in the original VCF file VCF Field info bytecodeList The INFO information of the current SV in the original VCF file CSV INDEX line int The line number of the current SV in the original VCF file CSV INDEX chr int[N] If the current SV is a complex SV, record the chromosomes where all the split SVs are located ANNOTATION INDEX indexes int[N] Record the intervals of lines related to the current SV and various annotations In the above description, we emphasize the concepts of \"splitting\" and \"assembling\" because we split all SVs into multiple single intervals. Each single - interval SV after splitting is called a Standardized Decomposition SV (SDSV). The specific principles of splitting and reconstruction are as follows: Based on the above principles, we provide an example of splitting from the original VCF file: Why use SDFA Compared with existing tools, SDFA has the following advantages: SDFA provides a systematic solution to fundamentally solve the problems of large-scale SV basic analysis. It can efficiently handle complex SV types, such as nested SVs, while other tools often fail to correctly parse these complex SVs. SDFA significantly outperforms existing tools in terms of speed and efficiency, especially on large - scale datasets. SDFA can collaborate with tools such as Plink to conduct SV-based GWAS research and explore SV at the population level. Functions and Outstanding features of SDFA? Efficient SV data storage and retrieval: Achieved through the SDF format. Consistent and robust SV merging algorithm: Capable of handling large-scale sample data. Fast and memory-efficient SV annotation: Using the indexed sliding window algorithm. Novel and precise gene feature annotation: Using the Numerical Annotation of Gene Features (NAGF) method. Excellent performance: At least 17.64 times faster in SV merging speed and at least 120.93 times faster in annotation speed. Ability to parse and annotate complex SVs: The only tool that can correctly handle nested complex SVs. High scalability: Successfully processed 895,054 SVs from 150,119 individuals in the UK Biobank dataset, while other methods failed. Parallel processing capability: Can utilize multi-threading to improve processing speed. Flexible customization features: Such as user-defined filtering conditions and annotation resources. "},"download.html":{"url":"download.html","title":"Download and Install","keywords":"","body":"Download and Install SDFA is an application developed based on Oracle JDK 8. Our software can be used on any computer device that supports or is compatible with Oracle JDK 8. Users need to download and install Oracle JDK or Open JDK first. For Apple Silicon devices, zulu JDK can be used as an alternative. Resource Type Path Software https://github.com/Overinterested/SDFA/blob/master/SDFA.jar Source Code https://github.com/Overinterested/SDFA/tree/master/src API Document https://pmglab.top/SDFA/api Example Files 1. VCF test files：1100 VCF files2. VCF pedigree files：http://data.schatz-lab.org/jasmine/HG002Trio/UnmergedVCFs/3. Annotation resource files： https://zenodo.org/records/13293672 Install Software # install SDFA software wget https://github.com/Overinterested/SDFA/blob/master/SDFA.jar # run SDFA java -jar SDFA.jar Update Log [!UPDATE|label:2024/05/20] Release the first version of SDFA, version number 1.0, Github repository address: TODO "},"usage/sdf_build/vcf2sdf.html":{"url":"usage/sdf_build/vcf2sdf.html","title":"Build SDF Archives","keywords":"","body":"Build SDF from VCF SDFA provides the conversion from VCF (Variant Calling Format) files to SDF (Standardized Decomposition Format) files for standard diploid species, aiming to achieve efficient storage of SV data, rapid access and location of SV and its related attributes, and compression of VCF files. Quick Start In the command line, use the following command to build an SDF archive for the genomic VCF file: java -jar sdfa.jar vcf2sdf [options] Of course, the above command line supports converting GZ and BGZ compressed VCF files into SDF files. Here are a few simple examples: Build SDF from single VCF file [!NOTE|label:Example 1] Build an archive using the example fileHG01258_HiFi_aligned_GRCh38_winnowmap.sniffles.vcf: java -jar sdfa.jar vcf2sdf -f ./HG01258_HiFi_aligned_GRCh38_winnowmap.sniffles.vcf -o ./ Build SDF from the fold [!NOTE|label:Example 2] Build an archive using a folder as input: java -jar sdfa.jar vcf2sdf -d ./data -o ./ Build SDF by specifying the Calling Type of VCF files [!NOTE|label:Example 3] Unlike the VCF files of SNPs, the SV VCF formats of different calling tools vary (mainly reflected in the INFO field), and these differences can affect the extraction of SV coordinate positions. Therefore, we have implemented the extraction of SV VCFs from 13 mainstream SV calling tools: CuteSV2、CuteSV、Debreak、Delly、NanoSV、Nanovar、Pbsv、Picky、Sniffles2、Sniffle、Svim、Svision、Ukbb In SDFA, the type of the selected Calling tool can be specified through --calling-type or-ct. It is worth noting that, by default or when an unknown ct is specified, the parsing will be carried out according to the standard SV VCF4.3. java -jar sdfa.jar vcf2sdf -ct cutesv -d ./data -o ./ API Document The API tool for converting VCF files to SDF files is SDSVManager. The usage examples are as follows: // Convert all VCF files and their GZ and BGZ compressed files in a certain folder int thread = 4; SDSVManager.of(\"inputDir\") .setOutput(\"outputDir\") .setCallingType(\"cuteSV\") .run(thread); "},"usage/sdf_build/file2sdf.html":{"url":"usage/sdf_build/file2sdf.html","title":"Build SDF from other files","keywords":"","body":"Build SDF From other file type Currently, the main recording and storage format of SV is the VCF file. Therefore, in SDFA, there is no command - line tool to directly build an SDF archive from other file formats. However, based on the extensive compatibility of SV files and the SV analysis tools developed subsequently, SDFA provides an API to build an SDF archive. SDFWriter build SDF Archives The SDFWriter is a class specifically designed to directly build an SDF by constructing SDFWriterRecord. In essence, it is a process of obtaining a record, setting the record, writing the record, resetting the record and ending the record. Therefore, users need to consider how to convert an input file format into an SDFWriterRecord record. Now, let's give an example of the use of SDFWriter: [!NOTE|label:Example 1] The essence of SDFWriterRecord is an SV record. Therefore, we need to construct an SV record and manually write it into a file. public static void main(String[] args) throws IOException, InterruptedException { String[] names = new String[1000]; for (int i = 0; i The above example is to construct an SV and write it. Therefore, for an input file in any format, what the user needs to construct is the process of converting Line → Record. API Document Specifically, users can check the SDF archive built by the SDFWriter class. "},"usage/sdf_toolkit/gui.html":{"url":"usage/sdf_toolkit/gui.html","title":"SDF Operation Toolkit","keywords":"","body":"Graphical User Interface For SDF To facilitate the viewing of SDF files, SDFA has designed a graphical interface that makes full use of the block and column features of SDF files and has the following characteristics: Local Scan: Only scan the content being viewed, making use of block and column features. Page Navigation: Support fast navigation with low memory usage. Custom Display: Allow setting the viewing method when the user has a custom encoding method. Complete Information: Display the source information of VCF header information, SV information, and file size. "},"usage/sdf_toolkit/filter.html":{"url":"usage/sdf_toolkit/filter.html","title":"Filter Mode","keywords":"","body":"Filter Mode Compared with single - nucleotide polymorphism (SNP), the filtering and screening of SV often lack a unified standard, and the screening attributes are more diverse (possibly including INFO, QUAL, GT fields, etc.). To perform more comprehensive SV filtering, SDFA has a rich set of built - in filtering functions. Specifically, it mainly covers two aspects of filtering: Genotype level：Set quality control information for genotype filtering - set those that do not meet the conditions as./. SV level：Set a custom function to filter multiple VCF fields such as CHR and ID of the current SV record The specific instructions are java -jar sdfa.jar filter -d [input_dir] -o [output_dir] [options] Genotype Filter Use the following instructions for filtering: --filter-gty The above command line filters the specified format_attr, and function is an expression that returns a boolean value. To facilitate users to directly operate on the values of the quality control attributes of each genotype, we directly use value in the function to represent the value of a single genotype. Now, let's take an example: [!NOTE|label:Example 1] We filter the genotypes of all SDF files in the ./data folder. The filtering quality control attribute is GQ, and the filtering condition is (int)value>20. The output is saved to the ./folder: java -jar sdfa.jar filter -d ./data -o ./ --filter-gty GQ (int)value>20 If the SDF file is a multi-sample file, perform the above-mentioned conditional filtering on the genotypes of each sample in sequence. To simplify the conversion logic and reduce the storage space of metric field data, we encapsulate common quality control attributes into specific types to facilitate users to operate directly. The details are as follows: Storage Class Storage Type Value Type Quality control attributes Example Example Explanation SingleIntValueBox int List Single int value DP,DR,DVGQ,MD,PP --filter-gty DP (int)values>10 Set the genotype to./ when the DP attribute value is TwoIntValueBox int List IntList instance composed of 2 int values AD,RA --filter-gty AD ((IntList)values).get(0)>10 Set the genotype to./ when the first int value of the AD attribute value is ThreeIntValueBox int List IntList instance composed of 3 int values PL --filter-gty PL ((IntList)values).get(0)>10 Set the genotype to./ when the first int value of the PL attribute value is SingleStringValueBox string List Single string value FT others --filter-gty FT (string)values.equals(\\\"TRUE\\\") Set the genotype with the FT attribute not being TR SV Filter Similar to the instructions for genotypes, the instructions for SV filtering are as follows: --filter-sv The above command line filters the specified sv_attr, and function is a JAVA expression that returns a boolean value. Now, let's take an example: [!NOTE|label:Example 2] We filter for IMPRECISE in the INFO field. When this field exists, we filter the SV: java -jar sdfa.jar -d ./data -o ./ --filter-sv IMPRECISE value!=null In the above example, the sv_attr is IMPRECISE. In the SDFA design, there is a certain order for the positioning and acquisition of sv_attr. The following is the comprehensive filtering order: INDEX Match Field Value Type Value Description 1 CHR string Obtain the chromosome name of SV 2 ID Bytes Obtain the ID value of SV 3 REF Bytes Obtain the REF value of SV 4 ALT Bytes Obtain the ALT value of SV 5 QUAL Bytes Obtain the QUAL value of SV 6 FILTER Bytes Obtain the FILTER value of SV 7 INFO List Obtain all INFO field values of SV 8 FORMAT List Obtain the quality control values of all genotypes of SV 9 GT CacheGenotypes Obtain the genotypes of all samples under SV 10 LEN int Obtain the LEN value of SV 11 SDF_FIELD_NAME Object Obtain the value of SV in SDF_FIELD_NAME (for details, please refer to the SDF structure) 12 INFO_ATTR Bytes Obtain the value of SV in INFO where the KEY is INFO_ATTR 13 FORMAT_ATTR Bytes Obtain the value of SV in FORMAT where the KEY is FORMAT_ATTR 14 UNKNOWN ERROR . For detailed value retrieval, you can refer to the getV function of SDFRecordWrapper. [!TIP|label:Filter Order] It is worth noting that in the actual code execution, the SV Level filtering will be carried out first, and then the Genotype Level filtering. "},"usage/sdf_toolkit/extract.html":{"url":"usage/sdf_toolkit/extract.html","title":"Sample Extraction","keywords":"","body":"Sample Extraction In large datasets, it is often necessary to extract fixed samples for downstream analysis. For example, the UKB's Whole genome GraphTyper SV data [interim 150k release] is a merged 150k dataset. When we need case - control samples for certain diseases, we need to perform extraction operations. After the VCF file is converted to an SDF file, use the following command to extract the SDF: java -jar sdfa.jar extract [options] Here we use the PED file as the input file to extract the required samples from the population SDF file. At the same time, SDFA provides some basic SV screening functions for the SV after sample extraction. [!NOTE|label:Example 1] We extract all SDF files in the input folder: java -jar sdfa.jar extract -d ./data -ped ./ped.ped -o ./ Program parameters Grammar：extract -d input_dir -o output_dir -ped ped_path Java-API: edu.sysu.pmglab.sdfa.toolkit.SDFExtract About: Extract samples from PED in multiple SDF files Parameters： *--output, -o Set the output folder. Format：-o *--dir, -d Set the input folder. Format：-d *--ped-file, -ped Set the PED file. Format：ped --thread, -t. Set the thread numbers. --max-maf Set the maximum proportion of genotypes in the extracted samples --min-maf Set the minimum proportion of genotypes in the extracted samples API Document The API tool for extracting SDF files is SDFExtract, and the usage examples are as follows: SDFExtract.of(file.toString(), sdfExtractProgram.pedFile, FileUtils.getSubFile(sdfExtractProgram.outputSDFDir, file.getName()) ) .setMaxMAF(sdfExtractProgram.maxMaf) .setMinMAF(sdfExtractProgram.minMaf) .submit(); "},"usage/sdf_toolkit/concat.html":{"url":"usage/sdf_toolkit/concat.html","title":"Concatenate Multiple SDF","keywords":"","body":"Concatenate Mmultiple SDF files Concatenation means adding new variant sites line by line. The most common scenario is to reassemble the variant sites of the same group of subjects that are scattered in different files (stored according to chromosomes, the number of variant sites, and file size) back into a single file (for example: the UKB's Whole genome GraphTyper SV data [interim 150k release] data). Use the following command to concatenate SDF files: java -jar sdfa.jar concat [options] When merging multiple files, the sample names of all files will be checked, and a two-way merge sort will be used for merging in each thread. During the merging process, the coordinates are ensured to be in order and the META information is continuously updated. [!NOTE|label:Example 1] The following is the merging of 3 single - sample files (assuming here that the sample names of the 3 files are the same): java -jar sdfa.jar concat -d ./data -o ./ -t 4 "},"usage/sdfa_toolkit/annotate.html":{"url":"usage/sdfa_toolkit/annotate.html","title":"Functional Annotation","keywords":"","body":"Functional Annotation To further locate susceptible SVs and explore their biological mechanisms, it is often necessary to perform functional annotation on SVs. In order to quickly conduct functional annotation for multiple samples and multiple resources, SDFA has designed the index sliding window algorithm shown in the figure below to accelerate the annotation process. At the same time, in order to perform customized annotation for multiple resources, SDFA has designed the post-annotate annotation method. The output file can be customized through the configuration file shown in the figure below: Configuration File SDFA draws on the post annotation concept of the Vcfanno tool and defines a configuration file for configuring annotation resources and output results. The specific explanation is as follows: Parameters Parameter Explanation Required or not(* represents required, . represents optional) [[annotation]] The start identifier for a new annotation resource * file The full path of the annotation resource file * type Type of annotation resource(3 types are supported: gene, interval, and svdatabase) * names Column names of the output results . fields Column names of the input file . opts Functions corresponding to the output columns Needs to match the size of names Here we explain the above parameters through an example: [!NOTE|label:Example 1] Next, we will operate on the interval_1.txt file. This file is of the interval type, and its general content is: Elements between single lines are separated by \\t. Now we want to obtain the COL1 column related to SV, so we set fields=[\"COL1\"] here. At the same time, we want the output column to be named name, so we set names=[\"name\"]. The final configuration file is as follows: [[annotation]] file=/Users/wenjiepeng/Desktop/SDFA_4.0/test/annotation/data/interval_1.txt type=interval names=[\"name\"] fields=[\"COL1\"] opts=[\"concat\"] After completing the above configuration file, we perform annotation through the following command: java -jar sdfa.jar annotate \\ --config /Users/wenjiepeng/Desktop/SDFA_4.0/test/annotation/data/config.txt \\ -t 4 -d /Users/wenjiepeng/Desktop/SDFA_4.0/test/vcf \\ -o /Users/wenjiepeng/Desktop/SDFA_4.0/test/annotation/res The results after annotation are as follows: Among them, the fileID corresponds to different SV files in this folder (/Users/wenjiepeng/Desktop/SDFA_4.0/test/vcf). Annotation Resources SDFA supports integrating external databases for SV annotation, provided that the external data files meet the following basic format requirements: The tab character (\\t) must be used as the delimiter. The annotation file must contain a header line, and the column names should start with the number sign (#) (lines starting with ## will be ignored). In addition, when SDFA introduces interval annotation files and SV database files to annotate existing SV data, the following specific conditions must be met: For interval annotation files, the following requirements must be met: | Column | Name | Type | Example | | ------ | ---------------- | ------- | ------- | | 1 | Chromosome | String | chr1 | | 2 | Start Position | Integer | 1000 | | 3 | End Position | Integer | 3000 | | 4 | [Feature 1 Name] | String | V1 | | … | … | … | … | For SV database files, the following requirements must be met: | Column | Name | Type | Example | | ------ | ---------------- | ------- | ------- | | 1 | Chromosome | String | chr1 | | 2 | Start Position | Integer | 1000 | | 3 | End Position | Integer | 3000 | | 4 | SV Length | Integer | 2000 | | 5 | SV Type | String | DEL | | 6 | [Feature 1 Name] | String | V1 | | … | … | … | … | "},"usage/sdfa_toolkit/ngaa.html":{"url":"usage/sdfa_toolkit/ngaa.html","title":"Numeric Gene Feature Annotation","keywords":"","body":"Numeric Gene Feature Annotation Numeric Gene Feature Annotation (NAGF) is a new annotation result proposed by SDFA for SV. NAGF uses 8 bits to represent the affected gene feature regions (such as exons, introns), and 5 bytes to represent the proportion of the affected region of each gene feature within a single SV in the gene. The 8 bits form the feature bits, and the 5 bytes form the coverage range. The following is the detailed information about NAGF: Feature Bits: This byte makes full use of 8 bits to represent different functional regions of a gene. The first bit indicates whether the gene is a protein - coding gene. It represents the affected exons, promoters, UTRs, introns, and nearby regions respectively, where 0 indicates not affected and 1 indicates affected. It also represents complete coverage through copy - number variation (CNV) and inversion. Note that similar ideas can use more bits to represent more subtle features, such as 5'UTR and 3'UTR. Number of Coverage Bytes: These five bytes in this study represent the percentage of SV - affected areas (ranging from 0 to 100) in five gene - feature regions: exons, promoters, UTRs, introns, and nearby regions. The final result is as follows: GENE_NAME:Value:[xxxx， xxx，xx，xx，xxx，x] [!NOTE|label:Example 1] Use NAGF of SDFA to annotate the example folder and output it to the tmp folder in the user's home directory: java -jar /Users/wenjiepeng/projects/sdfa_latest/SDFA.jar ngf \\ -dir /Users/wenjiepeng/Desktop/tmp/sdfa_test/sdf_builder/vcf2sdf \\ -f /Users/wenjiepeng/projects/sdfa_latest/resource/hg38_refGene.ccf \\ -o /Users/wenjiepeng/Desktop/tmp/sdfa_test/sdf-toolkit/sdfa-nagf "},"usage/sdfa_toolkit/gwas.html":{"url":"usage/sdfa_toolkit/gwas.html","title":"SV-based GWAS","keywords":"","body":"SV-based GWAS SV-based GWA studies have emerged. Since PLINK integrates many practical statistical testing methods, SDFA provides an SV-based GWA workflow for further exploring vulnerable SVs. Taking the UKBB database as an example, when the sample size is massive, considering the huge amount of data, the overall SV data is split into multiple VCF files, with each file storing a part of the entire SV data. At the same time, since PLINK has integrated many GWA-related statistical analysis tools, SDFA provides the SDF2Plink tool to convert SDF files into Plink input files for subsequent SV-based GWA analysis. SDFA has established a VCF ⇒ SDF ⇒ Plink pattern, which mainly consists of 4 processes: VCF2SDF: Convert VCF to SDF files and perform operations such as SV filtering and information extraction. SDFConcat: Integrate multiple SDF files into one SDF file. SDFExtract: Extract partial sample information from the integrated SDF file. SDF2Plink: Convert the extracted samples into Plink file format, namely .fam,.bed and .bim files. [!NOTE|label:Example 1] Here we take an example. First, integrate multiple SDF files in ./test/resource/gwas folder. Then, extract the samples from ./test/resource/gwas/sample.ped file. Finally, convert them into PLINK files. And use the PLINK files for analysis. java -jar ./SDFA.jar gwas \\ -dir ./test/resource/gwas \\ -o ./test/resource/gwas/output \\ --ped-file ./test/resource/gwas/sample.ped \\ --concat \\ -t 4 # plink ## 1. filter ./test/resource/gwas/plink2 --bfile ./test/resource/gwas/output/ \\ --geno 0.2 \\ --mind 0.8 \\ --hwe 1e-6 \\ --maf 0.05 \\ --make-bed \\ --out ./test/resource/gwas/output/geno_0.2_mind_0.8_maf_0.05 ## 2. association ./test/resource/gwas/plink2 --bfile ./test/resource/gwas/output/geno_0.2_mind_0.8_maf_0.05 \\ -adjust \\ --glm \\ allow-no-covars \\ --out ./test/resource/gwas/output/geno_0.2_maf_0.05_res "},"usage/sdfa_toolkit/merge.html":{"url":"usage/sdfa_toolkit/merge.html","title":"SDFA Toolkit","keywords":"","body":"Sample Merge In order to further conduct larger-scale SV research, SDFA has developed a cohort-wide merging algorithm for SV sample merging at the population scale level. This algorithm is based on the SDF file and performs k-way merging on the basis of considering all SV data in an orderly manner: Currently, SDFA uses position to perform SV merging among samples, as follows: SDFA maintains an ordered list of SVs of the same type and location, and adds SVs of the same type in ascending order of the minimum position. When a newly added SV (marked as $SV_{new}$) does not meet the merging conditions, all SVs in the current list will be popped and merged into one SV. The default merging conditions are as follows: ∣Posfirst−Posnew∣Threshold∣Endfirst−Endnew∣Threshold \\begin{aligned} |Pos_{first}-Pos_{new}|​∣Pos​first​​−Pos​new​​∣Threshold​∣End​first​​−End​new​​∣Threshold​​ [!NOTE|label:Example 1] SDFA merges the VCF files (including compressed files and SDF files) in the specified folder and outputs the merged results to the specified folder. The command line is as follows: java -jar sdfa.jar -d ./data -o ./ "}}